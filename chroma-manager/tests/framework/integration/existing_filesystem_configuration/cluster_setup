#!/bin/bash -ex

spacelist_to_commalist() {
    echo $@ | tr ' ' ','
}

[ -r localenv ] && . localenv

CLUSTER_CONFIG=${CLUSTER_CONFIG:-"$(ls $PWD/existing_filesystem_configuration_cluster_cfg.json)"}

. $CHROMA_DIR/chroma-manager/tests/framework/utils/cluster_setup.sh

# need to remove the chroma repositories configured by the provisioner
pdsh -l root -R ssh -S -w $(spacelist_to_commalist $CHROMA_MANAGER) "exec 2>&1; set -xe
if $MEASURE_COVERAGE && [ -f /etc/yum.repos.d/autotest.repo ]; then
    cat << \"EOF\" >> /etc/yum.repos.d/autotest.repo
retries=50
timeout=180
EOF
    $PROXY yum install -y python-setuptools python2-coverage
fi
if [ -f /etc/yum.repos.d/autotest.repo ]; then
    rm -f /etc/yum.repos.d/autotest.repo
fi" | dshbak -c
if [ ${PIPESTATUS[0]} != 0 ]; then
    exit 1
fi

# And install the MFL required repos
pdsh -l root -R ssh -S -w $(spacelist_to_commalist $ALL_NODES) "exec 2>&1; set -xe
yum-config-manager --enable addon-epel\$(rpm --eval %rhel)-x86_64
if ! yum repolist | grep addon-epel; then
    yum -y install epel-release
fi
yum-config-manager --add-repo https://copr.fedorainfracloud.org/coprs/managerforlustre/manager-for-lustre/repo/epel-7/managerforlustre-manager-for-lustre-epel-7.repo

yum-config-manager --add-repo http://mirror.centos.org/centos/7/extras/x86_64/
ed <<EOF /etc/yum.repos.d/mirror.centos.org_centos_7_extras_x86_64_.repo
/enabled/a
gpgcheck=1
gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7
.
wq
EOF

if [[ \$HOSTNAME = *vm*[29] ]]; then
    build_type=client
else
    build_type=server
fi
yum-config-manager --add-repo https://build.whamcloud.com/lustre-b2_10_last_successful_\$build_type/
sed -i -e '1d' -e \"2s/^.*$/[lustre-\$build_type]/\" -e '/baseurl/s/,/%2C/g' -e '/enabled/a gpgcheck=0' /etc/yum.repos.d/build.whamcloud.com_lustre-b2_10_last_successful_\${build_type}_.repo

yum-config-manager --add-repo https://downloads.whamcloud.com/public/e2fsprogs/latest/el7/
sed -i -e '1d' -e '2s/^.*$/[e2fsprogs]/' -e '/baseurl/s/,/%2C/g' -e '/enabled/a gpgcheck=0' /etc/yum.repos.d/downloads.whamcloud.com_public_e2fsprogs_latest_el7_.repo

yum -y install distribution-gpg-keys-copr
if ! ls /usr/share/distribution-gpg-keys/copr/copr-*manager-for-lustre*; then
    rpm --import https://copr-be.cloud.fedoraproject.org/results/managerforlustre/manager-for-lustre/pubkey.gpg
fi
$LOCAL_CLUSTER_SETUP" | dshbak -c
if [ ${PIPESTATUS[0]} != 0 ]; then
    exit 1
fi

# Install pdsh on storage nodes
pdsh -l root -R ssh -S -w $(spacelist_to_commalist ${STORAGE_APPLIANCES[@]}) "yum -y install pdsh" | dshbak -c

# Install and setup manager
scp $ARCHIVE_NAME $CHROMA_DIR/chroma-manager/tests/utils/install.exp root@$CHROMA_MANAGER:/tmp
ssh root@$CHROMA_MANAGER "#don't do this, it hangs the ssh up, when used with expect, for some reason: exec 2>&1
set -ex
yum -y install expect pdsh
# Install from the installation package
cd /tmp
tar xzvf $ARCHIVE_NAME
cd ${ARCHIVE_NAME%.tar.gz}

# Execute the create_installer script and distribute the install procedure to the storage nodes
./create_installer zfs
./create_installer ldiskfs
pdcp -l root -R ssh -w $(spacelist_to_commalist ${STORAGE_APPLIANCES[@]}) lustre-zfs-${TEST_DISTRO_NAME}${TEST_DISTRO_VERSION%%.*}-installer.tar.gz $INSTALLER_PATH
pdcp -l root -R ssh -w $(spacelist_to_commalist ${STORAGE_APPLIANCES[@]}) lustre-ldiskfs-${TEST_DISTRO_NAME}${TEST_DISTRO_VERSION%%.*}-installer.tar.gz $INSTALLER_PATH

if ! expect ../install.exp $CHROMA_USER $CHROMA_EMAIL $CHROMA_PASS ${CHROMA_NTP_SERVER:-localhost}; then
    rc=\${PIPESTATUS[0]}
    cat /var/log/chroma/install.log
    exit \$rc
fi

cat <<\"EOF1\" > /usr/share/chroma-manager/local_settings.py
import logging
LOG_LEVEL = logging.DEBUG
EOF1

# https://github.com/pypa/virtualenv/issues/355
python_version=\$(python -c 'import platform; print \".\".join(platform.python_version_tuple()[0:2])')
if $MEASURE_COVERAGE; then
    cat <<\"EOF1\" > /usr/share/chroma-manager/.coveragerc
[run]
data_file = /var/tmp/.coverage
parallel = True
source = /usr/share/chroma-manager/
EOF1
    cat <<\"EOF1\" > /usr/lib/python\$python_version/site-packages/sitecustomize.py
import coverage
cov = coverage.coverage(config_file='/usr/share/chroma-manager/.coveragerc', auto_data=True)
cov.start()
cov._warn_no_data = False
cov._warn_unimported_source = False
EOF1
else
    # Ensure that coverage is disabled
    rm -f /usr/lib/python\$python_version/site-packages/sitecustomize.py*
fi"

# Install and setup chroma software storage appliances
pdsh -l root -R ssh -S -w $(spacelist_to_commalist ${STORAGE_APPLIANCES[@]}) "exec 2>&1; set -xe
# if this node uses the Intel proxies, make sure the agent is doing so
if [ -f /etc/profile.d/intel_proxy.sh ]; then
    echo \". /etc/profile.d/intel_proxy.sh\" > /etc/sysconfig/chroma-agent
fi

if [ -f /etc/yum.repos.d/autotest.repo ]; then
    cat << \"EOF\" >> /etc/yum.repos.d/autotest.repo
retries=50
timeout=180

EOF
    $PROXY yum -y install python-setuptools python2-coverage
    pushd /tmp/
    tar xzvf lustre-ldiskfs-el7-installer.tar.gz
    tries=0
    failed=true
    while \$failed && [ \$tries -lt 10 ]; do
        if ! bash -x lustre-ldiskfs/install; then
            yum --enablerepo=lustre clean metadata
            let tries+=1
        else
            failed=false
        fi
    done
    if \$failed; then
        exit 1
    fi
    popd
fi
# https://github.com/pypa/virtualenv/issues/355
python_version=\$(python -c 'import platform; print \".\".join(platform.python_version_tuple()[0:2])')
if $MEASURE_COVERAGE; then
    cat <<\"EOF\" > /usr/lib/python\$python_version/site-packages/.coveragerc
[run]
data_file = /var/tmp/.coverage
parallel = True
source = /usr/lib/python\$python_version/site-packages/chroma_agent/
EOF
    cat <<\"EOF\" > /usr/lib/python\$python_version/site-packages/sitecustomize.py
import coverage
cov = coverage.coverage(config_file='/usr/lib/python\$python_version/site-packages/.coveragerc', auto_data=True)
cov.start()
cov._warn_no_data = False
cov._warn_unimported_source = False
EOF
else
    # Ensure that coverage is disabled
    rm -f /usr/lib/python\$python_version/site-packages/sitecustomize.py*
fi

if $USE_FENCE_XVM; then
    # fence_xvm support
    mkdir -p /etc/cluster
    echo \"not secure\" > /etc/cluster/fence_xvm.key
fi

# Removed and installed a kernel, so need a reboot
sync
sync
nohup bash -c \"sleep 2; init 6\" >/dev/null 2>/dev/null </dev/null & exit 0" | dshbak -c
if [ ${PIPESTATUS[0]} != 0 ]; then
    exit 1
fi

source $CHROMA_DIR/chroma-manager/tests/framework/integration/utils/install_client.sh

# Install and setup integration tests
scp $CLUSTER_CONFIG root@$TEST_RUNNER:/root/cluster_cfg.json
ssh root@$TEST_RUNNER <<EOF
exec 2>&1; set -xe
$PROXY yum --disablerepo=\* --enablerepo=chroma makecache
$PROXY yum -y install chroma-manager-integration-tests

if $USE_FENCE_XVM; then
    # make sure the host has fence_virtd installed and configured
    ssh root@$HOST_IP "exec 2>&1; set -xe
    uname -a
    $PROXY yum install -y fence-virt fence-virtd fence-virtd-libvirt fence-virtd-multicast
    mkdir -p /etc/cluster
    echo \"not secure\" > /etc/cluster/fence_xvm.key
    restorecon -Rv /etc/cluster/
    cat <<\"EOF1\" > /etc/fence_virt.conf
backends {
	libvirt {
		uri = \"qemu:///system\";
	}

}

listeners {
	multicast {
		port = \"1229\";
		family = \"ipv4\";
		address = \"225.0.0.12\";
		key_file = \"/etc/cluster/fence_xvm.key\";
		interface = \"virbr0\";
	}

}

fence_virtd {
	module_path = \"/usr/lib64/fence-virt\";
	backend = \"libvirt\";
	listener = \"multicast\";
}
EOF1
    chkconfig --add fence_virtd
    chkconfig fence_virtd on
    service fence_virtd restart"
fi
EOF

$CHROMA_DIR/chroma-manager/tests/framework/integration/utils/wait_for_nodes.sh "${STORAGE_APPLIANCES[*]} $CLIENT_1"

echo "End cluster installation and setup."
