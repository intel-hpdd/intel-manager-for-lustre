# ExaScaler cluster configuration file.
# Copyright DDN, 2016

[global]

# You can provide a shadow configuration file, in order to store all your passwords.
# The esctl showall command uploads all the needed files to DDN's FTP server, which also
# contains the exascaler.conf file. For security purpose you can mention all your passwords into
# the shadow configuration file. Shadow file is /etc/ddn/exascaler-shadow.conf and
# it should have read and write permissions only for the user. Example of the shadow file can be
# found at /opt/ddn/es/example/exascaler-shadow.conf. To enable this behavior, enable this setting
# Only owner of shadow config file should have each of read and write permissions.
;shadow_conf: no

# List of filesystems exported from this cluster (space delimited)
fs_list: testfs

# List of emails to receive logging (comma delimited)
;email_list: Test User <test@example.com>
;email_domain: example.com
;email_relay: mail.example.com

# List of ntp servers to sync with
;ntp_list: ddn.ntp.server

# The timezone to be set.
;timezone: UTC

# Name of the EXAScaler cluster. This parameter is mandatory only in case of rest section is presented.
; cluster_name: es-cluster


# Extra hosts which are known about but aren't used to serve lustre.  Hosts
# listed here are managed by the same tools, are part of the cluster
# and are included in clush host lists.  'start' hosts are hosts who's IP's come
# before the fs hosts, 'end' hosts are hosts who's ip's come after
;extra_hosts_start: head
;extra_hosts_end: client0 client1

# Hosts listed here are managed by the same tools but not the part of
# corosync cluster and are not included in clush hosts lists.
;clients_list: client3 client4

# List of SFAs for this cluster(space delimited)
sfa_list: sfa12k es7k

# The directory where logs will be saved to.
# Defaults to /scratch/log and is always appended by the filesystem name.
# e2fsck/lfsck databases  and logs will be saved in the subdirectory ${fs_name}/fsck
# Large files are created in this directory so care should be taken choosing it.
# defaults to: /scratch/log
; log_dir: /scratch/log

# Pingd option activates a pair of resources: ifspeed-pingd.
# Ifspeed monitors a physical state of the NIC and it speed. Pingd resource
# will be activated only if ifspeed are active.
# If pingd is enabled, it will detect network failures and stop resources on
# the host with network issues. Monitored networks are those used by the MDS.
# Given by lustre internal constraints, all OSS are forced to support the
# networks of the MDS. However, in rare cases an OSS also might have
# additional network to clients, but not to the MDS (e.g. tcp on MDS,
# but tcp + o2ib on OSS). In that case only the networks available
# on the MDS will be monitored. Until further tested, pingd defaults to 'yes'.
pingd: yes

# Frequency of automatic MDT backups.
# Can be daily, weekly, loris or none
# In case of loris backup will be performed each day at 2a.m. by default.
; mdt_backup: none

# Path where loris will keep all backups. Currently supported only ext4 and btrfs devices.
; mdt_backup_dir: /scratch

# Path where kdump will create the crash dump
# This is optional parameter and defaults to /scratch/crash
;kdump_path: /scratch/crash

# By default all the volume groups are activated at node reboot. But if you are using
# HA the volume groups are being managed by HA and so we should exclude them from
# the auto activation list for volume groups. Setting this to 'auto' will
# automatically detect which volume groups to auto activate. You can also explicitly specify
# the list of volume groups like 'vg_activation_list: VolGrp0000 VolGrp0001'.
# Setting this to 'none' will not add anything to auto activation list which means all
# the volume groups will be activated on reboot. The default value is 'auto'.
# Use 'none' option to stick to the old behavior.
;vg_activation_list: auto

# This option enables the use of encrypted password for SFA user
# By enabling this, user need not specify the password in exascaler.conf
# The password has to be passed to a utility 'es_encrypt_sfa_passwd' which
# encrypts the password. Default is 'plain-text'
;password_policy: encryption

[sysctl_defaults]
# List of all the kernel parameters to be set on all hosts
;net.ipv4.tcp_window_scaling: 2
# recommended kernel parameter vm.min_free_kbytes=2097152 to improve system stability and performance under high load
vm.min_free_kbytes: 2097152

[sysctl oss0]
# List of all the kernel parameters to be set for host oss0
;net.ipv4.tcp_window_scaling: 1

# List of Lustre parameters which can be set using lctl set_param command
# These settings will be set permanently
[set_param_tunings]
;osc.*.max_dirty_mb: 512
;osc.*.max_rpcs_in_flight: 64
;osc.*.max_pages_per_rpc: 1024
;ost.OSS.ost.threads_min: 116
;ost.OSS.ost_io.threads_min: 116
;obdfilter.*.brw_size: 4
;debug: 0

#Here is how you can enable end-to-end data integrity based on T10-PI algorithms
#E2EDI server
;obdfilter.*.checksum_t10pi_enforce: 1
#E2EDI client
;llite.*.checksum_pages: 1
;osc.*.checksum_type: t10ip4K

# List of Lustre parameters which can be set using lctl conf_param command
# These settings will be set permanently.
[conf_param_tunings]
;testfs.llite.max_read_ahead_mb: 128
;testfs.llite.max_read_ahead_per_file_mb: 64


# Description of SFA, which is a part of the cluster.
[sfa sfa12k]

# List of SFA controllers.
controllers: 10.52.16.44 10.52.16.45

# User for SFA
# defaults to: user
;user: user

# Password for SFA
# defaults to: user
;password: user

[sfa es7k]
controllers: 10.52.16.46 10.52.16.47

[HA]

# What type of HA we are using on this system.
# Valid options are:
# none.       No HA configured
# corosync.   rhel6,7 systems using corosync
# For new systems is most likely selection here is 'corosync'
type: corosync

# Corosync option.
# This specifies the mode of redundant ring, which  may  be  none,
# active,  or  passive.   Active replication offers slightly lower
# latency from transmit to delivery in faulty network environments
# but  with  less  performance.   Passive  replication  may nearly
# double the speed of the totem protocol if the  protocol  doesn’t
# become  cpu bound.  The final option is none, in which case only
# one  network  interface  will  be  used  to  operate  the  totem
# protocol.
rrp_mode: passive

# Corosync option.
# This specifies that HMAC/SHA1 authentication should be used to authenticate all corosync messages.
# Enabling this option adds a encryption header to every message sent by totem which reduces total throughput.
# Also encryption and authentication consume extra CPU cycles in corosync.
# According to /usr/share/man/man5/corosync.conf.5.gz this option is deprecated and combination of 'crypto_hash'
# and 'crypto_cipher' should be used instead.
# The default is on.
; secauth: on

# Corosync option.
# This specifies which HMAC authentication should be used to authenticate all messages.
# Valid values are none (no authentication), md5, sha1, sha256, sha384 and sha512.
# The default is sha1.
; crypto_hash: sha1

# Corosync option.
# This specifies which cipher should be used to encrypt all messages.
# Valid values are none (no encryption), aes256, aes192, aes128 and 3des.
# Enabling crypto_cipher, requires also enabling of crypto_hash.
# The default is aes256.
; crypto_cipher: aes256

# The network maximum transmit unit to use (only for corosync).
# This entity has strong relationships with window_size and max_messages and shoult be set by system engineers only.
# The default is 1500
; netmtu: 1500

# Corosync option.
# This constant specifies the maximum number of messages that may be sent on one token rotation.
# If all processors perform equally well, this value could be large (300), which would  introduce
# higher latency from origination to delivery for very large rings.  To reduce latency in large rings(16+),
# the defaults are a safe compromise.  If 1 or more slow processor(s) are present among fast processors,
# window_size should be no larger then 256000 / netmtu to avoid overflow of the kernel receive buffers.
# The user is notified of this by the display of a retransmit list in the notification logs.
# There is no loss of data, but performance is reduced when these errors occur.
# The default is 50 messages.
; window_size: 50

# Corosync option.
# This constant specifies the maximum number of messages that may be sent by one processor on receipt of the token.
# The max_messages parameter is limited to 256000 / netmtu to prevent overflow of the kernel transmit buffers.
# The default is 17 messages.
; max_messages: 17

# Which transport type to use (only for corosync).
# Valid options are:
# multicast.   Use multicast to communicate
# unicast.     Use unicast.
# The default is multicast.
; transport: multicast

# Allow disabling of pacemaker quorum policy. By default it should be enabled
# and set to 'freeze', but it MUST be disabled on systems with 2 hosts only
# or on the SFA10000E if only a single couplet is used, as
# then only n/2 systems will be  available on a controller failure. But it
# should be enabled again, if a further system is added (for example
# external MDS) or another SFA10000E couplet is added. Default is 'stop'.
# Set to 'ignore' on a SFA10000E or system with 2 hosts only.
# Allowed values are 'stop', 'ignore' and 'freeze'
no_quorum_policy: freeze

# Multicast port.
# If running multiple corosync clusters on the same subnet you'll need to change
# this to avoid conflicts.
; mcastport: 5405

# Corosync nics
# One or two corosync rings which could be customized in host section. Allowed values are 'ring0' and 'ring1'.
; corosync_nics: ring0 ring1

# Start on boot
# Control if the HA software starts automatically on node boot.  With this enabled
# HA will both fail-over and fail-back resources as required. To prevent automatic fail-over and fail-back after node boot,
# this setting can be set to false which will require manual start of the HA process after node boot.
# The default value is true
; start_on_boot: true

# HA Groups
# Corosync and pacemaker have a hard time dealing with large clusters. With
# these parameters, you can split up a cluster into multiple groups. Failover
# peers should be in the same group or else they won't be able to failover to
# each other.
; ha_group_count: 2
; ha_group0: mds0 mds1 oss0 oss1 oss2 oss3
; ha_group1: oss4 oss5 oss6 oss7 oss8 oss9

# The time to wait (dampening) for further changes occur.
# Use this to prevent a resource from bouncing around the cluster when cluster nodes notice
# the loss of connectivity at slightly different times.
# Default value is 20
; dampen_ping: 40
; dampen_ifspeed: 60

# Pacemaker option.
# How long in seconds to wait for a STONITH action using this device to complete.
# Tune this option if you have a custom ipmi_delay and ipmi_power_wait options.
# The default is 60
; stonith_timeout: 60

# Pacemaker option.
# How long in seconds to wait for a lustre target start action to complete.
# (Default Value: 450)
; lustre_start_timeout: 450

[host_defaults]

# The nics to run corosync rings on. At least ring0 should be specified.
ring0: eth0
ring1: eth1

# Pacemaker stonith types, valid values are none, ipmi, ipmi-slow, ssh, sfa10ke
# and sfa_vm
# ipmi-slow should be used for the DRAC-Express (shared port) IPMI controllers
# sfa_vm should be used for embedded environment
# sfa10ke is deprecated option and should not be used by anyone.
stonith_type: ipmi

# Stonith username and password.
# For use with 'ipmi', 'ipmi-slow', 'sfa_vm' stonith_type.
;stonith_user: username
;stonith_pass: password

# IPMI delay, monitor and method. For use only with 'ipmi' stonith_type.
# Wait X seconds before fencing is started. (Default Value: 15)
;ipmi_delay: 20
# Method to fence. Valid values: onoff, cycle (Default Value: onoff)
;ipmi_method: cycle
# Monitoring interval in seconds for fence_ipmilan fence agent. (Default Value: 60)
;ipmi_monitor: 30

# Number of seconds to wait after issuing a power off or power on command.For use only with 'ipmi' stonith_type.
# Wait X seconds after on/off operation.(Default Value: 5)
;ipmi_power_wait: 20

# Emulated serial speed for serial-over-lan. set to none if no serial exists
;serial_speed: 115200
# Port to be used for serial-over-lan. Default is 'ttyS0'
;serial_port: ttyS0

# Host network interface devices (space delimited)
# This can be overridden in host-specific sections below
nic_list: eth0 eth1 ib0

# In case of channel-bonding, nic_list should include bond interface
# itself and its slaves
;nic_list: eth0 eth1 bond0

# For channel-bonding, what mode to use
;bonding_mode: 4

# For channel-bonding, what interfaces to use as slaves
;bond0_slaves: eth0 eth1

# Use special format like '<nic>-alias<idx>' to enable nic aliases. As
# example to set "ib0:0" alias we should use "ib0-alias0"
;nic_list: ib0 ib0-alias0

# The following options can be used to auto-generate IP addresses
# for each host node in the cluster.  Host nodes are determined by the
# mds_list and oss_list options in each file system specification.
# The IP addresses are incremented by 1, starting with mds hosts and then
# moving to oss hosts, for each file system.
#
# NOTE: Do not use a starting base address on an interface if any host
# in the cluster will be assigned an address in the host-specific sections
ipmi_ip_base: 198.51.100.20
eth0_ip_base: 198.51.100.10

eth0_netmask: 255.255.255.0
;eth0-alias0_netmask: 255.255.255.0

# This is optional for now but may be essential in future.
ipmi_netmask: 255.255.255.0

# Use this to add any additional network options to an interface
eth0_gateway: 198.51.100.1

eth1_netmask: 255.255.255.0

ib0_netmask: 255.255.255.0

# Any additional modprobe settings, one per line. LNET networks will be added
# automatically, however other modprobe options may be added here.
;modprobe_cfg:
;   options ksocklnd enable_irq_affinity=0
;   options lnet routes="o2ib4711 198.51.100.[231-232]@o2ib1; o2ib5 130.242.72.[227-228]@o2ib1" \
;        check_routers_before_use=1 router_ping_timeout=250 dead_router_check_interval=300 live_router_check_interval=251
;   options hfi1 krcvqs=4 sge_copy_mode=2 wss_threshold=70 max_mtu=10240

# Add new or replace existing kernel grub options.
; grub_args: spectre_v2=off nopti crashkernel=1024M

# The default networks used by the filesystem (space delimited)
# Entries should be of the form 'name(interface)' where 'name' is the name
# that will be assigned to the network and 'interface' is the device the
# network will run on.
lnets: o2ib(ib0) tcp(eth0)

# Network interface for external vip mapping.(mandatory setting)(could be overwritten in particular host section)
; rest_ext_nic: eth0
# Network interface for internal vip mapping.(mandatory setting)(could be overwritten in particular host section)
; rest_int_nic: eth0
# Primary interface for REST API usage. (Default is the first corosync ring)
# (could be overwritten in particular host section)
; rest_primary_nic: eth0
# Network interface for keepalived service.(Default is network interface for internal vip)
# (could be overwritten in particular host section)
; rest_keepalived_nic: eth0
# Path to a cert which could verify client cert. Mandatory and applicable only in case node have master role.
# (could be overwritten in particular host section)
; rest_cert_ca: /root/certs/rootca.crt
# Path to a CRL, signed by CA. Applicable only in case node have master role.
# (could be overwritten in particular host section)
; rest_cert_crl: /root/certs/rootca.crl
# Path to a server certificate.Mandatory and applicable only in case node have master role.
# (could be overwritten in particular host section)
; rest_cert_server: /root/certs/server/es-virt-rest.crt
# Path to a private key of the server certificate. Mandatory and applicable only in case node have master role.
# (could be overwritten in particular host section)
; rest_cert_server_key: /root/certs/server/es-virt2-rest.key


# File System Options
# This is a list of options, one per file system

[fs testfs]

# Underlying filesystem type. Supported values: ldiskfs, zfs.
# Note that in current implementation all Lustre filesystems
# of the cluster must use the same backfs type.
; backfs: zfs
backfs: ldiskfs

# Should the MDT fail back to the primary MDS when it can?
# This should be enabled in cases where there is a performance
# difference when serving MDTs from different servers, for example
# when there are multiple filesystems defined in a active/active
# configuration or on the embedded platform where the MDT VD
# will be associated with a home controller and hence VM.  This
# option should also be enabled when running the MGT on a passive
# MDS node to ensure that MGT/MDT are not co-located and hence
# preventing the use of IR on MDT failover.
# This option can be disabled on single-filesystem, block storage
# systems where IR is not required.
# The disadvantage of enabling this option is that there will be
# a failover event when the primary MDT rejoins the HA cluster if
# it is enabled.
# The default value is to disable for single-filesystem, single-MDT
# systems.
; mdt_failback: yes

# Hostname Lists
# A complete list of managed hostnames can be obtained by
# concatenating these lists and removing duplicates
# The first server listed for the mds is considered the primary.
# The mgs_list variable is optional, if not set then the first
# 2 nodes from mds_list will be used for the mgs. Setting mgs_list
# is only valid when mgs_standalone is enabled.
; mgs_list: mgs0 mgs1
mds_list: mds0 mds1
oss_list: oss0 oss1 oss2 oss3

# The number of osts per oss, can be overridden on a per host basis
default_ost_count: 100

# With the Distributed Namespace (DNE) feature, we can support multiple MDTs per MDS.
# MDTs have active-active failover now.
# The number of MDTs per MDS can be overridden on a per host basis.
default_mdt_count: 2

# DoM feature aims at balancing IO performance for file systems that have both small
# and large (of more than a few megabytes) files.
# The default value is "no"
# To enable DoM
;dom_enabled: yes

# If DoM is enabled, sets the maximum DoM file size for ldiskfs.
# The default is 64K, which overrides Lustre’s default maximum DoM component size of 1MB
;dom_max_file_size=128

# mke2fs options for the MDT
;mdt_mke2fs_opts: -i 2560 -O project

# mke2fs options for the OSTs
# if neither ost_mke2fs_opts nor tune_ost_mke2fs_opts are specified, then OSTs will be formatted with the recommended mke2fs
# options: '-i 131072'  for size < 128TB  and '-N 1073741824' for size>=128TB.
# the flex_bg and -G option put descriptor groups together for shorter mounts
# We no longer need explicitly set ldiskfs "flex_bg" feature; mkfs.lustre command includes by default.
;ost_mke2fs_opts: -m1 -i 131072 -O project

# Here is how you can specify tune_ost_mke2fs_opts parameter depending on OST size:
#     for [size_condition] apply ['mke2fs_options']
# keyword 'size' must be used to describe size_condition
# supported size_condition operators:  '<' , '<=', '>', '>=', '=='
# supported size units: B, KB, MB, GB, TB, PB
# size_condition examples: size==90TB , size>=600TB, 128GB < size < 1TB
# mke2fs_options must be enclosed in single quotes
# mke2fs_options examples: '-i 131072', '-N 1073741824' ,'-N 2147483648'
# if tune_ost_mke2fs_opts is not specified or specified but OST size doesn't match size_condition, then ost_mke2fs_opts will be used.
# if tune_ost_mke2fs_opts is specified and OST size matches the size_condition, then the specified tune_ost_mke2fs_opts will be used.
# if both "ost_mke2fs_opts" and "tune_ost_mke2fs_opts" are specified in es config, then es_mkfs will use
# "tune_ost_mke2fs_opts" only if size condition matches, otherwise will use "ost_mke2fs_opts".
# if multiple conditions are specified for tune_ost_mke2fs_opts (comma separated), then mke2fs_options of the first matched size_condition will be used.
# if neither ost_mke2fs_opts nor tune_ost_mke2fs_opts are specified, then OSTs will be formatted with the recommended mke2fs
# options: '-i 131072'  for size < 128TB  and '-N 1073741824' for size>=128TB.
#examples:
#        tune_ost_mke2fs_opts: for size == 90TB apply '-i 131072'
#        tune_ost_mke2fs_opts: for 360GB < size < 2TB apply '-i 262144'
#example with multiple conditions (comma separated):
#        tune_ost_mke2fs_opts: for size == 150TB apply '-i 131072', for size == 750TB apply '-N 1073741824', for size > 750TB apply '-N 2147483648'
;tune_ost_mke2fs_opts: for size == 750TB apply '-N 1073741824'

# parameters added to tunefs.lustre command
mdt_opts: --param mdt.identity_upcall=NONE
# Since Lustre 2.4 *.quota_type=ug goes by default. Use it only in case of Lustre version < 2.4
;mdt_opts: --param mdt.quota_type=ug
;ost_opts: --param ost.quota_type=ug

# server-specific parameters added to mount.lustre command
; mgt_mount_opts: max_sectors_kb=0
; mdt_mount_opts: abort_recov
; ost_mount_opts: max_sectors_kb=0,abort_recov

# OST's LNET restriction parameters for tunefs.lustre command
# Use this options to configue OST based LNET balancing.
# Formula:
#     <lnet_name>_ost: 'pool_list'
;tcp1_ost: 0 2 4
;o2ib3_ost: 1 3 5

# path to the base MDT device (MDT and MGT will be on LVM
# and only LVM will use the base MDT device) and OST devices (NOT on LVM)
# defaults to /dev/mapper, but on systems without multipath, /dev/ddn
# should be used
;mdt_base_device_path: /dev/ddn
;ost_device_path: /dev/ddn

# The size of MDT and MGT filesystems to create
# This size is used by LVM during filesystem creation
mdt_size: 100g
mgs_size: 500m

# This is the number of LUNs that are going to make up the MDT. If it is
# greater than 1, than the MDT will be RAID-0 striped across the LUNs. If
# mdt_parts is 1, then the disk should be named <fsname>-MDS. If it is greater
# than 1, they should be named fs_mdt0_s0, fs_mdt0_s1, fs_mdt0_s2, etc.
; mdt_parts: 1

# should we create an MGT as part of this filesystem
mgs_internal: yes

# Should the MGT be on a separate storage device to the MDT?
# If enabled, this allows the MGT and the MDT to run on different
# hosts, thereby allowing the use of IR (imperative recovery)
# for the MDT that is now running separately and faster failover times.
# This requires the creation of a separate storage device for the MGT
# so there is no ability to enable this after the filesystem has been
# created.
# The default is 'no' for backwards compatibility reasons, it should
# be enabled for all new systems.
mgs_standalone: yes

# If the mgs isn't internal to this filesystem then specify the name of the local
# filesystem it is internal to.
; mgs_fs: demofs

# defaults to 5s
; mmp_update_interval: 5

# Represents the list of OST pools to be created for this filesystem
;pools: fast_pool

# example of overriding lnets and nic_list
[host mds0]
;ipmi_ip: (uses _base instead)
;eth0_ip: (uses _base instead)
nic_list: eth0 eth1
lnets: tcp(eth0)
eth1_ip: 203.0.113.10
peers: mds1
# Overrides mdt_device_path option of fs section. Must be equal on all peer hosts
;testfs_mdt_device_path: /dev/ddn
# We can override the default mdt count setting for a filesystem using
# the <fs_name>_mdt_count property
# We can specify the mdt index for a filesystem to be mounted on a particular
# mds using <fs_name>_mdts property
# By default if these properties are not set the MDT count will be taken from default_mdt_count
testfs_mdt_count: 3
;testfs_mdts: 0 1 2
grub_args: spectre_v2=off nopti

# Stonith peers are used to specify where stonith resources can be run.
# For IPMI based stonith all nodes have the ability to stonith all other nodes,
# on the embedded platform this isn't the case.

# stonith_primary_peers should be set to VMs which are on hosted the same
# controller and is the preferred (quickest) option for stonith.
;stonith_primary_peers: oss0 oss1
# stonith_secondary_peers should be set to VMs which are hosted on the same couplet
# but the other controller, these also have the ability to stonith VMs but should
# not be used if both controllers are running correctly.
;stonith_secondary_peers: mds1 oss2 oss3

# Embedded specific setting, the OID of this VM.  This must be specified for stonith
# to work correctly.  This setting is unique per controller only, not per couplet.
; oid: 0x10000000

# Host SFA list needs to be set in case of "sfa_vm" stonith type.
host_sfa_list: sfa12k

[host mds1]
nic_list: eth0 eth1
lnets: tcp(eth0)
eth1_ip: 203.0.113.11
peers: mds0
;testfs_mdt_device_path: /dev/ddn
testfs_mdt_count: 1
;testfs_mdts: 3
;stonith_primary_peers: oss2 oss3
;stonith_secondary_peers: mds0 oss0 oss1
;oid: 0x10000000

[host oss0]
eth1_ip: 203.0.113.12
ib0_ip: 192.0.2.10
peers: oss1 oss2 oss3
# Overrides ost_device_path option of fs section. Must be equal on all peer hosts
;testfs_ost_device_path: /dev/ddn
;stonith_primary_peers: mds0 oss1
;stonith_secondary_peers: mds1 oss2 oss3
# ost indices overwrite automatic OST assignment
;testfs_osts: 0-3
;oid: 0x10000001

# example showing overridden ost_count
[host oss1]
eth1_ip: 203.0.113.13
ib0_ip: 192.0.2.11
peers: oss0 oss3 oss2
;testfs_ost_device_path: /dev/ddn
;stonith_primary_peers: mds0 oss0
;stonith_secondary_peers: mds1 oss2 oss3
;testfs_ost_count: 3
# ost indices overwrite automatic OST assignment
;testfs_osts: 4-7
;oid: 0x10000002

[host oss2]
eth1_ip: 203.0.113.14
ib0_ip: 192.0.2.12
peers: oss3 oss0 oss1
;testfs_ost_device_path: /dev/ddn
;stonith_primary_peers: mds1 oss3
;stonith_secondary_peers: mds0 oss0 oss1
# ost indices overwrite automatic OST assignment
;testfs_osts: 8-11
;oid: 0x10000001

[host oss3]
eth1_ip: 203.0.113.15
ib0_ip: 192.0.2.13
peers: oss2 oss1 oss0
;testfs_ost_device_path: /dev/ddn
;stonith_primary_peers: mds1 oss2
;stonith_secondary_peers: mds0 oss0 oss1
# ost indices overwrite automatic OST assignment
;testfs_osts: 12-15
;oid: 0x10000002

[host rbh]
eth1_ip: 203.0.113.16
ib0_ip: 192.0.2.14

# Defines the pool definition
# here fast_pool is the name of the pool
# When used in a particular fs the pool will be created with the name pool_name for that fs
# The pool x for a fs y can be accessed as y.x
# ost_list defines the list of OST's to be included in this pool
;[pool fast_pool]
;ost_list: OST[0-3]

[host client3]
eth1_ip: 203.0.113.17
ib0_ip: 192.0.2.17

# The EXAScaler web UI (ESUI) configuration options
;[ESUI]
# Specify either “yes” to enable configuration of ESUI or “no” to disable it (default is “no”)
;enabled: yes
# Set a dedicated IP address for use by the web UI. This IP address moves between servers in case of failover.
# The setting is required if ESUI is enabled.
;ip: 1.2.3.4
# Specify the size of ESUI partition on mgs VG (default is 80GB)
;size: 120GB
# Optionally specify the network size that the ip belongs to (for example, “24” for a /24 network)
;cidr: 24
# If the ip is not easily routable from one of the network interfaces, this setting ensures that the correct interface is used
;nic: eth0

# Section to define REST API specific settings
; [rest]
# List of nodes which should be configured with master role. (space delimited)
; master_nodes: mds0 mds1
# IP of external vip.
; ext_vip: 198.51.100.100
# Mask of external vip
; ext_mask: 20
# IP of internal vip.
; int_vip: 198.51.100.110
# Mask of internal vip
; int_mask: 20
# Virtual router id for keepalived service. Should be unique for any cluster in network segment
; ka_vr_id: 220
# FQDN of REST API external interface.
; ext_vip_fqdn: es-virt-rest
# Organization unit field in client certificate Only clients with mentioned field will be successfully authentificated.
; auth_ou: REST API client